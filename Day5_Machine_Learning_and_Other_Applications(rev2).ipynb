{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day5 Machine Learning and Other Applications.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXSjKaHYkn5O"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ulV9FRKysO"
      },
      "source": [
        "import pandas\n",
        "import seaborn\n",
        "\n",
        "df1 = pandas.read_csv(\"https://raw.githubusercontent.com/YONESI-DBIS/DS_Lecture/main/UB_Data.csv\")\n",
        "print(df1)\n",
        "\n",
        "df1_desc = df1.groupby('Age')['Income'].describe()\n",
        "df1_desc = df1_desc.reset_index()\n",
        "print(df1_desc)\n",
        "\n",
        "df2 = seaborn.load_dataset('tips')\n",
        "print(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c3o783lL0WC"
      },
      "source": [
        "## 군집화(Clustering) k-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osNsQPAhL3hu"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 데이터 준비\n",
        "clustering_data = df2[['total_bill','tip']]\n",
        "\n",
        "# 정규분포(mean = 0, std. = 1)로 표준화\n",
        "sc = StandardScaler()\n",
        "clustering_data_scaled = sc.fit_transform(clustering_data)\n",
        "\n",
        "# Clustering 수행\n",
        "kmeans = KMeans(n_clusters = 4, random_state = 0)\n",
        "clusters = kmeans.fit(clustering_data_scaled)\n",
        "\n",
        "print(clusters.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th98ObRDOer5"
      },
      "source": [
        "# 원 데이터에 클러스터 레이블 추가\n",
        "clustering_data['cluster_label'] = clusters.labels_\n",
        "print(clustering_data)\n",
        "\n",
        "# 클러스터링 결과 확인\n",
        "clustering_data.groupby('cluster_label').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YASjpSI-QO9U"
      },
      "source": [
        "# 클러스터링 결과 시각화\n",
        "\n",
        "import seaborn\n",
        "\n",
        "chart = seaborn.lmplot(x = 'total_bill', y='tip', data=clustering_data, fit_reg=False, hue='cluster_label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsDFFi9uqxRE"
      },
      "source": [
        "## 주성분분석(PCA, Principal Component Analysis )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1MqWOpgTFxT"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "clustering_ubdata = df1[['Age', 'Income', 'CCAvg', 'Mortgage']]\n",
        "clustering_ubdata_scaled = sc.fit_transform(clustering_ubdata)\n",
        "\n",
        "# 주성분분석 \n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(clustering_ubdata_scaled)\n",
        "\n",
        "# 주성분만 포함하도록 데이터 변환\n",
        "clustering_ubdata_scaled_pca = pca.transform(clustering_ubdata_scaled)\n",
        "\n",
        "print(clustering_ubdata_scaled)\n",
        "print(clustering_ubdata_scaled_pca)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzRvKRnTW_e5"
      },
      "source": [
        "## 결정트리(Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ch9otLwXBX5"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "feature = df1[['CCAvg','Income']]\n",
        "feature = feature.to_numpy()\n",
        "\n",
        "decision = df1['CDAccount']\n",
        "decision = decision.to_numpy()\n",
        "\n",
        "tree = DecisionTreeClassifier().fit(feature, decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYfhWCAzEp50"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
        "    ax = ax or plt.gca()\n",
        "    \n",
        "    # Plot the training points\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
        "               clim=(y.min(), y.max()), zorder=3)\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # fit the estimator\n",
        "    model.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                         np.linspace(*ylim, num=200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # Create a color plot with the results\n",
        "    n_classes = len(np.unique(y))\n",
        "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
        "                           levels=np.arange(n_classes + 1) - 0.5,\n",
        "                           cmap=cmap, clim=(y.min(), y.max()),\n",
        "                           zorder=1)\n",
        "\n",
        "    ax.set(xlim=xlim, ylim=ylim)\n",
        "\n",
        "\n",
        "visualize_classifier(DecisionTreeClassifier(), feature, decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov-zaQBGasS3"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V50PeUXawcD"
      },
      "source": [
        "tree = DecisionTreeClassifier().fit(X, y)\n",
        "\n",
        "visualize_classifier(DecisionTreeClassifier(), X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rUtApHlnZVc"
      },
      "source": [
        "## 심층학습(Deep Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCWuH-d8o_F8"
      },
      "source": [
        "MNST 데이터셋을 사용한 handwriting recognition\n",
        "\n",
        "https://sdc-james.gitbook.io/onebook/4.-and/5.1./5.1.3.-mnist-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZw74n1mnc7Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB7luCzOpU6a"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_6yF9BaFP9u"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxIABaavktnG"
      },
      "source": [
        "# Other Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6hjbgtzpaUP"
      },
      "source": [
        "## 정규 표현식 (Regular Expression)\n",
        "\n",
        "- 문자 클래스 [ ] : 해당 문자중 한 개의 문자와 매치 ( - 를 사용하여 범위로 지정할 수도 있음)\n",
        "    - ^는 not의 의미\n",
        "    - [a-zA-Z]: 알파벳 모두, [0-9]: 숫자 모두\n",
        "    - \\d : 숫자\n",
        "    - \\D : not 숫자\n",
        "    - \\s : whitespace 문자\n",
        "    - \\S : not whitespace 문자\n",
        "    - \\w : 문자+숫자\n",
        "    - \\W : not 문자+숫자\n",
        "- Dot(.): 줄마꿈 문자(\\n)를 제외한 any 문자\n",
        "\n",
        "- 반복(*): 바로 앞의 문자가 0번에서 무한대번 반복\n",
        "- 반복(+): 바로 앞의 문자가 1번에서 무한대번 반복\n",
        "- 반복({m,n}, ?)\n",
        "    - {m}: 바로 앞의 문자가 m번 반복\n",
        "    - {m,n}: 바로 앞의 문자가 m번에서 n번 반복\n",
        "    - ?: 바로 앞의 문자가 있어도 되고 없어도 됨. 즉, {0,1}과 동일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppuL0rbbpnBP"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2cfF8OB1TJr"
      },
      "source": [
        "- match() : 문자열의 처음부터 정규식과 매치되는지 조사  -> 매치되면 match 객체 리턴 / 매치되지 않으면 None 리턴\n",
        "- search() : 문자열 전체를 검색하여 정규식과 매치되는지 조사  -> 매치되면 match 객체 리턴 / 매치되지 않으면 None 리턴\n",
        "- findall() : 정규식과 매치되는 모든 substring을 리스트로 리턴\n",
        "- finditer() : 정규식과 매치되는 모든 substring을 반복 가능한 객체로 리턴 (반복 가능한 객체는 match 객체들을 반복적으로 리턴)\n",
        "- sub(): 정규식과 매치되는 모든 substring을 다른 string으로 바꿈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlCV7Tzw1VAy"
      },
      "source": [
        "import re\n",
        "\n",
        "p = re.compile('[a-z]+')\n",
        "\n",
        "m = p.match(\"python\")\n",
        "print(m)\n",
        "\n",
        "m = p.match(\"3 python\")\n",
        "print(m)\n",
        "\n",
        "m = p.search(\"3 python\")\n",
        "print(m)\n",
        "\n",
        "p = re.compile('[a-z]+')\n",
        "\n",
        "m = p.match(\"python\")\n",
        "if m:\n",
        "    print('Match found: ', m.group())\n",
        "else:\n",
        "    print('No match')\n",
        "    \n",
        "#축약도 가능\n",
        "m = re.match('[a-z]+', \"python\")\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNIYLWo71yjR"
      },
      "source": [
        "import re\n",
        "\n",
        "p = re.compile('[a-z]+')\n",
        "\n",
        "result = p.findall('life is too short')\n",
        "print(result)\n",
        "\n",
        "result = p.findall('Life is too Short')\n",
        "print(result)\n",
        "\n",
        "#p = re.compile('[a-zA-Z]+')\n",
        "#result = p.findall('Life is too Short')\n",
        "#print(result)\n",
        "\n",
        "#text_data = 'life 13 is too 12 short!!!'\n",
        "#text_modified = re.sub('[^a-zA-Z]+',' ',text_data)\n",
        "#print (text_modified)\n",
        "\n",
        "#text_data = '본 연구는 자카드 유사도(Jacard Similarity)가 다른 집합 유사도(Set Similarity)와 어떤 차이가 있는지를 보인다.'\n",
        "#text_modified = re.sub(\"\\(.+\\)\", '', text_data)\n",
        "#print(text_data)\n",
        "#print(text_modified)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_cBMPeOp4vp"
      },
      "source": [
        "## 간단 텍스트 마이닝(Simple Text Mining)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjRUBO9mp7J-"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "text_data = '''\n",
        "In Gloucester, Massachusetts, Ruby Rossi is the only hearing member of her family: her parents Frank and Jackie and older brother Leo are all culturally deaf. She assists with the family fishing business and plans to join it full-time after finishing high school. Ruby auditions for the school choir, run by Bernardo Villalobos (or Mr. V), but when she is called upon to sing, she panics and runs away. She later returns to Mr. V and explains that she was bullied for talking funny as a child. Mr. V accepts Ruby into the choir after hearing her beautiful voice and encourages her to be more confident.\n",
        "Mr. V pairs Ruby with Miles, a fellow student, for a duet at the upcoming choir recital. Their first performance goes poorly as they each prepare separately; Mr. V insists that they get together on their own to practice. Ruby invites Miles to her house to practice, but they are interrupted by Frank and Jackie loudly having sex in the next room over. Ruby later hears classmates in the cafeteria mocking the incident behind her back; Miles apologizes for spreading the story, but she wants nothing to do with him. She eventually forgives him and they resume their practice while kindling a relationship.\n",
        "Meanwhile, Frank and Leo struggle to make ends meet with the fishing business as new fees and sanctions are imposed by the local board. At a board meeting, Frank stands and announces that he is starting his own company to get around the new restrictions and sell his fish on his own, inviting other local fishermen to join in. The family struggles to get the company off the ground, relying on Ruby to talk to people and spread the word.\n",
        "Mr. V encourages Ruby to audition for Berklee College of Music and offers her private lessons to prepare. Ruby joins him for the lessons, but becomes increasingly busy helping her family with the business. Mr. V grows irritated with her constantly being late and making excuses, canceling their lessons. He chastises her for wasting his time and accuses her of not caring enough about music.\n",
        "While fishing one day, Frank and Leo are intercepted by the Coast Guard after failing to respond to ship horns and radio calls. They are fined and have their fishing licenses revoked for their negligence; they appeal and manage to get their license back on the condition that they have a hearing person on board with them at all times. Ruby announces to the family that she is foregoing college and will join the business full-time. Her parents are supportive, but Leo reacts angrily, insisting that they can manage their own problems without Ruby's help.\n",
        "Ruby's family attends her choir recital, and while they cannot hear her sing, they notice the positive reception from the audience around them. That night, Frank asks Ruby to sing a song for him while he feels her vocal cords, growing emotional. The entire family then drives to Boston with Ruby for her Berklee audition; they are not supposed to enter the audition hall, but they sneak up to the balcony to watch anyway. Ruby is nervous at first but gains confidence when she sees her family; she signs along with the song so they can understand what she is saying.\n",
        "Some time later, Ruby is accepted to Berklee; she shares the news with her family and Mr. V, who are all excited for her, before asking Miles to visit her in Boston sometime. Meanwhile, the hearing workers in the family's fishing business have been learning sign language, allowing them to interpret for the family. Ruby's friend Gertie drives her to Boston for college as her family sees them off; Ruby signs \"I love you\" to them as they drive away.\n",
        "'''\n",
        "\n",
        "# 한번만 수행해도 됨 #########\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "##############################\n",
        "\n",
        "stopWords = set(stopwords.words(\"english\"))\n",
        "\n",
        "stopWords.add('ruby')\n",
        "stopWords.update(['ruby', 'leo', 'frank'])\n",
        "#print(stopWords)\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "onlyEnglish = re.sub(r\"[^a-zA-Z]+\", \" \", text_data)\n",
        "print(onlyEnglish)\n",
        "\n",
        "print(\"===\")\n",
        "\n",
        "english_word_token = word_tokenize(onlyEnglish.lower())\n",
        "print(english_word_token)\n",
        "\n",
        "\n",
        "tagged_english_word_token = pos_tag(word_tokenize(onlyEnglish.lower()))\n",
        "print(tagged_english_word_token)\n",
        "\n",
        "'''\n",
        "only_noun_english_word_token = list()\n",
        "for word in tagged_english_word_token:\n",
        "  if word[1] == 'NN':\n",
        "    only_noun_english_word_token.append(word[0])\n",
        "print(only_noun_english_word_token)\n",
        "\n",
        "english_word_token = only_noun_english_word_token\n",
        "'''\n",
        "\n",
        "english_word_token_stop = list()\n",
        "for word in english_word_token:\n",
        "  if word not in stopWords:\n",
        "    english_word_token_stop.append(word)\n",
        "\n",
        "print(english_word_token_stop) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3w1HDy3-EiU"
      },
      "source": [
        "count_result = Counter(english_word_token_stop)\n",
        "print(count_result)\n",
        "\n",
        "word_count = dict()\n",
        "\n",
        "for tag, counts in count_result.most_common(30):\n",
        "    if(len(str(tag))>3):\n",
        "        word_count[tag] = counts\n",
        "        print(\"%s : %d\" % (tag, counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTiLOeUn_pNE"
      },
      "source": [
        "# 히스토그램 표시 \n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.xlabel(\"word\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.grid(True)\n",
        "\n",
        "sorted_Keys = sorted(word_count, key=word_count.get, reverse=True)\n",
        "sorted_Values = sorted(word_count.values(), reverse=True)\n",
        "\n",
        "plt.bar(range(len(word_count)), sorted_Values, align='center')\n",
        "plt.xticks(range(len(word_count)), list(sorted_Keys), rotation='85')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWMoXY58_-oR"
      },
      "source": [
        "# Word Cloud\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "word_cloud = WordCloud(background_color='ivory', stopwords=stopwords, width=800, height=600)\n",
        "cloud = word_cloud.generate_from_frequencies(word_count)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAGpu_bJsta8"
      },
      "source": [
        "## 웹크롤링(Web Crawling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgAfy1mIdIa_"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.yonsei.ac.kr/sc/support/corona_notice.jsp?mode=view&article_no=198507&board_wrapper=%2Fsc%2Fsupport%2Fcorona_notice.jsp&pager.offset=0&board_no=752'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "  html = response.text\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  content = soup.select_one('#jwxe_main_content > div.jwxe_board > div > dl > dd > div.cont_area')\n",
        "  print(content.get_text())\n",
        "\n",
        "else : \n",
        "  print(response.status_code)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXyznGvkgIZS"
      },
      "source": [
        "## 웹크롤링(Web Crawling) + 워드크라우드(Word Cloud)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjS4AjMBgeNK"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "\n",
        "url = 'https://www.imdb.com/title/tt10366460/plotsummary?ref_=tt_stry_pl'\n",
        "#url = 'https://www.imdb.com/title/tt3480822/plotsummary?ref_=tt_stry_pl'\n",
        "#url = 'https://www.imdb.com/title/tt9376612/plotsummary?ref_=tt_stry_pl'\n",
        "#url = 'https://www.imdb.com/title/tt2294629/plotsummary?ref_=tt_stry_pl'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "  html = response.text\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  content = soup.select_one('#plot-synopsis-content')\n",
        "#  print(content.get_text())\n",
        "\n",
        "else : \n",
        "  print(response.status_code)\n",
        "\n",
        "\n",
        "stopWords = set(stopwords.words(\"english\"))\n",
        "\n",
        "stopWords.add('ruby')\n",
        "stopWords.update(['ruby', 'leo', 'frank'])\n",
        "stopWords.update(['romanoff', 'vostokoff', 'belova', 'dreykov', 'dreykov', 'wenwu', 'shang', 'anna', 'elsa', 'kristoff', 'duke', 'olaf'])\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "onlyEnglish = re.sub(r\"[^a-zA-Z]+\", \" \", content.get_text())\n",
        "\n",
        "print(\"===\")\n",
        "\n",
        "english_word_token = word_tokenize(onlyEnglish.lower())\n",
        "#print(english_word_token)\n",
        "\n",
        "\n",
        "tagged_english_word_token = pos_tag(word_tokenize(onlyEnglish.lower()))\n",
        "#print(tagged_english_word_token)\n",
        "\n",
        "only_noun_english_word_token = list()\n",
        "for word in tagged_english_word_token:\n",
        "  if word[1] == 'NN':\n",
        "    only_noun_english_word_token.append(word[0])\n",
        "#print(only_noun_english_word_token)\n",
        "\n",
        "english_word_token = only_noun_english_word_token\n",
        "\n",
        "english_word_token_stop = list()\n",
        "for word in english_word_token:\n",
        "  if word not in stopWords:\n",
        "    english_word_token_stop.append(word)\n",
        "\n",
        "count_result = Counter(english_word_token_stop)\n",
        "#print(count_result)\n",
        "\n",
        "word_count = dict()\n",
        "\n",
        "for tag, counts in count_result.most_common(30):\n",
        "    if(len(str(tag))>3):\n",
        "        word_count[tag] = counts\n",
        "        print(\"%s : %d\" % (tag, counts))\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "word_cloud = WordCloud(background_color='ivory', stopwords=stopwords, width=800, height=600)\n",
        "cloud = word_cloud.generate_from_frequencies(word_count)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}